---
title: 'Notes of CS231n:Neural network 1'
date: 2018-02-28 15:17:09
toc: true
tags:
 - 学习笔记
 - 深度学习
---

## 人工神经元

  其实神经元这个翻译有点唬人，有些学者也不喜欢借用某些生物词语来蹭热度，他们直接就把所谓的人工神经元称之为单元（unit），结合人工神经网络来看的话，你可以很容易的推测出，它是由这个所谓的人工神经元组成的。在一个人工神经网络中，神经元间会有一种单向的影响，因为人工神经网络可以看做是一张有向无环图。下图是一个常见的粗糙模型：

<div align="center">

<img width="600px" src="\img\QQ图片20180214181505.png"/>

<img src="\img\QQ图片20180214180333.jpg"/>

</div>

从这张图中你可以看到，一个人工神经元有几个主要的部分，如果你把它看做是有向无环图里的一个节点，那么你可以观察到它的每一个入度，分别对应一个不同的输入和一个自身权重的乘积（w和x都是向量或者矩阵，这里准确来说应该是**点积**），所有入度的点积的累加加上节点对应的偏差值（偏置）(Σwixi+b）代入所入节点的激活函数，图中就是f，对应为f(Σwixi+b),然后这个输出结果（**激活率**）顺着节点的出度流入下一层网络中的神经元。

  现在你大概知道什么是所谓的人工神经元，如果硬要跟生物神经元扯上一毛钱关系的话，你学过高中生物，大概会知道突触，树突，突触小泡之类的，你也许能把这个和两个神经细胞里通过突触来传递兴奋的机制联系起来...然后对别人吹嘘一波，但是我是做不到的，可能是我不怎么了解生物吧。 

<!--more-->

## 激活函数

  认识了人工神经元之后，我们就知道了这里面有一个叫做**激活函数（activation function）**的重要概念，激活函数的选择对最终训练出来的模型影响是很大的，如果你以前看过一些公众号中推送的文章，可能会经常看到Sigmoid和Tanh这样的字眼，这是都是激活函数的名称，但是你不要看着这些名字这么奇怪就觉得这个函数会不会很复杂啊，其实稍微用点脑子思考一下就知道激活函数不能是多么复杂的函数，因为我们最后需要通过反向传播这样的算法去计算梯度，从而进行参数调优，如果这个激活函数是非常复杂的，这个梯度就不太好算了，激活函数通常都是非线性函数，目的是为了模型有更强的表达能力，详情可以[参考这里](https://www.zhihu.com/question/22334626)，排名前几的答案基本上把这个问题讲清楚了。这里我记录3个激活函数：

### S型函数

  S型函数就是Sigmoid，如果在你学高等数学或者微积分的时候，肯定是接触过这个函数的，这个函数由于它在平面直角坐标系上的曲线而得名。即：

> σ（x）=1/(1+e^(-x))

![QQ图片20180214194819](\img\QQ图片20180214194819.png)

### Tanh函数

  Tanh函数是有S型函数转化过来的零中心函数，Tanh比S型函数更受欢迎，但是他们存在着饱和问题，即在接近极值的区间，梯度几乎为0，训练模型的时候使用反向传播算法，一个接近0的梯度反向传播下去，小数相乘会以指数形式收缩到0。

> tanh=2σ(2x)-1=2/(1+e^(-2x))-1

![QQ图片20180214195043](\img\QQ图片20180214195043.png)

### ReLU函数

目前最流行的激活函数,非常简单粗暴有效,名字的由来是Rectified Linear Unit->ReLU

> ReLu(x)=max(0,x)

![QQ图片20180214200631](\img\QQ图片20180214200631.png)

## 神经网络结构

我们通过一个普通的神经网络来展开对它的层结构认知，比如下面这个两层的网络：

![NN](\img\NN.jpg)

  一般来说输入层是不算做层数的，所以说这个网络是2层，即一个隐层和输出层，当然有些网络有很多隐层，我们在这个图片上看到的是一个有向无环图，但是经过前面的理解，你知道实际上它是一个做矩阵点积的数学函数。

  在这个普通的网络中，它的层都是**全连接层**，全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。

  在神经网络中，输出层是没有激活函数的，因为它用来完成分类的打分。有时候我们要表示一个神经网络的尺寸，比如上面那个神经网络是怎么样的尺寸呢，它有着4（隐层）+2（输出层）=6个神经元，因为是全连接的，它有着3\*4+4\*2=20个权重，以及4+2=6个偏置，权重和偏置都是可学习的，那么我们训练这个神经网络的时候需要学习这个26个参数，以最优化算法求得他们的最优值。这样看起来似乎比较清晰了，但是一般的卷积神经网络有着10-20层，1亿个可学习参数，要求这个1一个参数的最优值可不是一个简单的工程，这也是为什么大部分做深度学习的同学都处在调参的路上的原因了。

  是不是层数越多的神经网络越好呢?视情况而定，一般的神经网络就是3层网络，再增加层数很难增强它的表达力度，而卷积神经网络则不同，因为要提取层次化特征，所以深度就是一个很重要的问题，一般都是数十层之多，谈到这个问题，我们又有新的问题，如何设置神经网络的层数和尺寸。我们知道神经网络其实是一个函数，如果在有着相同激活函数的情况下，整个函数更为复杂，则函数的表达能力就能更强，但是也会引起**过拟合**等问题，**过拟合**是指网络对数据中的噪点拟合过甚，导致整个模型的泛化能力不强。

  即便大的网络会有过拟合问题，我们还是会使用更大一点的网络作为模型，因为防止过拟合还有其他的方法，比如**L2正则化或者dropout**，特别是正则化。因此我们没有必要去牺牲整个网络模型的表达能力去换去泛化能力。另外小网络的极小值很容易收敛到，但是这些极值的损失值一般都很高，不利于我们构建一个优秀的模型，而大的网络极小值更多，虽然更不容易收敛，但是我们可以有更多的选择，从而得到一个更好的模型。
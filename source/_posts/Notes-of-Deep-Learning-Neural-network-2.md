---
title: 'Notes of CS231n:Neural network 2'
date: 2018-03-01 10:26:43
toc: true
tags:
 - 学习笔记
 - 深度学习
---

  从上一篇博文，我们知道，神经网络其实就是一个一系列线性映射和非线性激活函数交织起来的评分函数，我们在输入层输入我们的数据，经过隐层的一系列点积和激活函数计算在输出层得到一个类型评分，然而要比较好地训练一个神经网络，我们需要更多的考虑，比如数据预处理，我们还需要考虑权重的初始值以及如何选择激活函数。

## 数据预处理

  我们通常会对**训练集**数据进行预处理，当然这可能有很多种形式，比如我们如果要去除一些噪声可能要进行数据清洗，这一部分这里先不介绍了，我们这里主要介绍归一化，归一化可以有利于避免模型训练时出现数值问题，还有利于网络快速收敛，更具体的优点你可以查看[这里](http://nnetinfo.com/nninfo/showText.jsp?id=37)。

  归一化（Normalization）即把数据的所有维度都归一化，使其数值范围都近似相等，这个种预处理操作只在确信不同输入特征有不同的输入范围时才有意义，对于图像处理中，像素的范围都是0-255，所以这样做对于图片数据意义不大，但是数据如果是表示一个人，（体重（kg），身高（m），年龄）,这样的数据显然60Kg的体重和1.7m的身高不太对等，这会产生一些数值问题，我们有必要进行归一化。这里有两种方式进行**归一化**：

先假设数据矩阵M，尺寸是N*D,N为样本数目，D是数据维度

第一种：对数据先做零中心化(zero-centered)，如果用numpy表示：M-=np.mean(X,axis=0)，然后我们对每个维度都除以它的标准差就可以了，M/=np.std(X,axis=0)。

第二种：对每个维度归一化使得每个维度的最大和最小值是1和-1。

<!--more-->

  实际上零中心化单独也可以看成是一种预处理操作，被称作是**均值减法**在实际中用的也比较多，另外由于近来BN层的引入，归一化在有BN层的情况下也就没有必要了，因为每次卷积后BN层会把数据拉到0均值1方差的分布（标准高斯分布）上，并且这个均值和方差还是更加精确的动态统计出来的，而非原始输入上统计，因此在BN引入后归一化没什么必要。

  我在前面就说过，数据预处理，归一化只是一种情况，还可能进行数据清洗，数据增强，白化和PCA等等操作，图片处理中有时候我们还会转换图片的格式，这也是一种预处理。预处理在模型训练中非常重要，有时候甚至决定着学习算法本身的好坏。

## 权重初始化

  要训练我们的网络，我们确定好我们需要一个怎么样的模型，然后构建一个最开始的网络，这样的话，网络中各种参数的初始值就显得非常重要，权重就是其中之一。

  一些人可能会认为把权重都设置为0比较好，然而仔细思考一下，如果一开始整个网络中每一个神经元的输出都是一样的（权重相同），那么反向传播中他们会输出同样的梯度，从而进行同样的更新，这显然就是不对的，你相当于复制了神经元。

  因为数据进行了适当的归一化，又要避免相同的参数更新，我们可以让权重的初始值接近于0，但是却又不等于0，为了得到不同的权重，我们可以以随机数产生器器来初始化我们的权重，这里我们产生的随机数可以服从正态分布或者均匀分布。需要注意的时，最好不要让随机值太小，因为太小的权重值会导致计算出的额梯度非常小，以至于一致我们在反向传播中的梯度信号。

  为了保证神经元起始时有近似相同的输出分布，我们必须想办法校准方差，一般的方法是采用1/sqrt(N)，来做校准，其中N是输入数据的数量，这样我们可以把输出数据的分布的方差校准到1，如果用numpy表示，即：w=np.random.randn(N)/sqrt(n)。

  有一种被称作**稀疏初始化** 的方法，将索引权重设置为0，但是为了避免相同的参数更新，每个神经元都同下一层固定数目的神经元随机连接，连接权重的数值由一个小的高斯分布生成。

  当前比较推荐的方法是在激活函数是ReLU的情况下使用，w=np.random.randn(N)/sqrt(2/n)来进行权重初始化，另外我们一般把偏置设置为0。

  上面介绍数据预处理的时候我们提到过一个被称为BN层(Batch Normalization)的东西，中文翻译是：**批量归一化**，它不仅可以避免**归一化**操作，也可以帮助我们解决初始化神经网络这个问题，我们通常会在全连接或者卷积层与激活函数之间添加一个BN层，让数据服从标准高斯分布，详细操作可以看这篇[文章](https://arxiv.org/abs/1502.03167)。

## 正则化

  我们在上一篇文章中知道了，正则化是避免过拟合的好方法，正则化也有很多种,比如最常见的L2正则化，以及你听的非常多的随机失活(dropout)方法。

### L1，L2正则化

1.L2正则化

  **L2正则化的思想是对大数值的权重向量进行严厉的惩罚，从而倾向于权重向量更加分散，即使网络更倾向于使用所有的特征。**实际操作是对网络中的每个权重w，向目标函数中增加一个**1/2（λ*w^2)**,λ是正则化强度，而乘上常数1/2是数学上的小技巧，即让求导后梯度变成λw，而非2λw。在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以**w += -lambda \* W**向着0线性下降。

2.L1正则化

  **L1正则化则倾向于让神经元最后使用它们最重要输入数据的稀疏子集，可看做特征选择**，它的实际操作是对每个w都像目标函数增加一个λ*|w|，这样的话，你可以知道求导的时候，附加项的导数就与w的符号有关，如果w为正，最优化更新后的w变小（相比与没有添加L1正则项时），如果为负则更新后的w变大，即尽可能使权重值为0。在w为0时，我们把取符号的sgn(w）看作是0，即sgn(0)=0,sgn(w>0)=1,sgn(w<0)=-1。

一般来说，实践更倾向于使用L2正则化，除非我们需要特征选择。

### 随机失活

  目前我们经常看到随机失活(dropout)的字眼，**其与L1，L2正则化，以及最大范式约束的方法互为补充**，实际操作过程中，随机失活即是让神经元以超参数p为概率被设置为0或者被激活,**p一般设置为0.5**，这样我们可以看做对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数。

![dropout](\img\dropout.jpg)

  但实际上我们更倾向于使用Inverted dropout，这是因为如果我们在训练网络时使用前向随机失活，比如：

```python
def train_step(X):
  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # 第一个随机失活遮罩
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # 第二个随机失活遮罩
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
```

  上面这段代码中有p的概率，神经元会被激活，而又1-p的概率神经元会被设置为0，然后流向下一层，那么我们可以知道在随机失活进行后，输出为px，而在我们的测试过程中并不存在随机失活层，所以神经元对输入都是可见的，那么为了保持同样的预期输出，我们必须在测试时把输出结果乘以p,来缩放激活函数输出结果，以防止测试输出数值范围大于训练输出，如：

```python
def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # 注意：激活数据要乘以p
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # 注意：激活数据要乘以p
  out = np.dot(W3, H2) + b3
```

  这样的话测试时要多乘一次p显得非常麻烦，为了让我们的测试效率更高，各种深度学习框架中实践的其实是inverted dropout,有翻译为**反向随机失活**，个人觉得这个翻译不太好，**其实它就是在训练阶段对激活函数输出值进行放大，即是放大1/p倍，而测试阶段则保持不变。**

```python
def train_step(X):
  # 3层neural network的前向传播
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # 第一个随机失活遮罩. 注意/p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # 第二个随机失活遮罩. 注意/p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3

def predict(X):
  # 前向传播时模型集成
  H1 = np.maximum(0, np.dot(W1, X) + b1) # 不用数值范围调整了
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

## 损失函数的选择

  我们知道在**分类问题中**两个最常见的分类器对应的损失计算：

  SVM->折页损失/平方折页损失（下图没列出）

  Softmax->交叉熵损失

![loss](\img\loss.jpg)

  如果我们面对的不是一个一般的分类问题，怎么办呢？上面的几种损失公式都只正确作用在每个样本只有一个正确的标签yi的假设成立的情况之下。但是你可以看到，在hexo博客中的一篇文章是可以打几个标签的，亦或者是新浪微博上的一张图片，比如你的女神正在日本旅游，发了一张在清水寺前不知道谁给它拍了一张照，可以打上美女，日本旅游，快乐等等几个标签。我们可以采取下面的一种损失评分策略：

![loss2](\img\loss2.png)

其中yij的值为1或者-1，它表示第i个样本是否被贴上第j个标签，如果是就是1，如果不是就是-1，这样的话，可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。

  再比如回归问题，比如你要预测北京的空气质量pm2.5的值，一般我们考虑计算预测值和真实值之间的损失，再用L2或者L1范式度量差异。





















